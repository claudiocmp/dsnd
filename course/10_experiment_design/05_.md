## Multiple Metrics

the probability of making at least one Type I error is given by $1−(1−\alpha_ind​)^n$. Therefore, we have a type-I error depending on how many metrics we analyse. We highlight two cases of alpha (probability of incorrectly not rejecting the null hypothesis) as $\alpha=0.05$ and $\alpha=0.01$

![](img\multiple_metrics_type_I_error.png)

We therefore introduct two ways of correcting it, with the assumption that **all metrics are independent (correlation = 0)**

  * Bonferroni correction: $\alpha_ind = \alpha_over / n$

  * Sidak correction: $\alpha_ind = 1 - (1 - \alpha_over)^{1 / n}$

![](img\multiple_metrics_bonferroni_correction.png)

In real life, evaluation scenarios are rarely so straightforward. **Metrics will likely be correlated in some way**, rather than being independent. If a positive correlation exists, then knowing the outcome of one metric will make it more likely for a correlated metric to also point in the same way. 
In this case, **the corrections above will be more conservative than necessary**, resulting in an overall error rate smaller than the desired level. (In cases of negative correlation, the true error rate could go either way, depending on the types of tests performed.)


### Experiment outcome

To declare an experiment a success

  * We might **need multiple metrics to show statistical significance** to call an experiment a success
  * There may be **different degrees of success** depending on which metrics appear to be moved by the manipulation
  * One metric may not be enough to make it worth deploying a change tested in an experiment

Being too conservative might lead to fail driving implementation of experimented changes. So, you can use use dummy test results, bootstrapping, and permutation approaches to plan significance thresholds

**NOTES:**
  * Reducing the individual error rate will make it harder for a truly significant effect to show up as statistically significant. That is, reducing the Type I error rate will also increase the Type II error rate – another conservative shift.
  * **these cautions also apply to invariant metrics!**
